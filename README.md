# PureView Project Suite ‚Äî AI Data Engineering Tools

This repository contains multiple sub-projects focused on improving data quality, automation, and intelligence gathering for AI-driven data engineering workflows.

---

## üìå Project 1: PureView

### üîç Why We‚Äôre Doing This

- **Quality at a Glance**: Instantly verify that every dataset has been cleaned to standard (e.g., duplicates removed, invalid rows dropped).
- **Time Saver**: Replaces manual script checks with one visual hub.
- **Reporting Ready**: Centralized metrics feed straight into reports.

---

### ‚úÖ What Success Looks Like

| **Metric**        | **Raw** | **Cleaned** | **% Change** |
|-------------------|--------:|------------:|-------------:|
| Total records     |    ‚Ä¢     |      ‚Ä¢       |       ‚Ä¢       |
| Duplicate rows    |    ‚Ä¢     |      ‚Ä¢       |       ‚Ä¢       |

---

### üîÑ Data Flow Overview

1. **Post-Cleaning Analysis Script**: Generates summary CSV per dataset.
2. **Lightweight Harvester App**: Ingests summary CSVs into SQLite.
3. **Python Dashboard**: Reads SQLite to render interactive charts and tables.
4. *(Optional)* **Nightly Export**: Cron job outputs master CSV for audit or PowerBI/Tableau users.

---

### üß± Scope of Work

- Finalize metric list & CSV schema
- Build harvester + SQLite loader; test with 3 datasets
- Create dashboard with filters, bar charts, and duplicate metrics
- User acceptance testing, polish, and documentation hand-off

---

## üîé Project 2: ReconX

### üß† What It Is

**ReconX** is an autonomous tool designed to automate, supplement, and scale the effort of discovering leaked datasets across the web.

---

### ‚ö†Ô∏è Why It Matters

- Leaked data often vanishes quickly
- Manual monitoring is limited in scale
- ReconX ensures real-time, continuous discovery with full traceability

---

### üõ†Ô∏è What It Does

- Crawls breach forums and file-sharing hosts using crafted queries
- Collects and catalogs metadata
- Logs every decision and action for full auditability

---

### üéØ Target Users

- Data Engineering Teams
- Threat Intelligence Teams

---

### üí° Strategic Benefits

- Automated discovery of leaked data
- Greater efficiency over manual efforts
- Transparent governance with structured logs

---

## üßΩ Project 3: Scrub AI (Data Cleaning Agent)

### üéØ Goal

Use AI to analyze a dataset sample and generate a Python data-cleaning script by auto-assigning variables and logic based on a provided template.

---

### üß© Key Features

1. **AI-Led Data Analysis**  
   - Identifies all columns
   - Flags key columns for cleaning logic

2. **Semi-Automated User-in-the-Loop Mode**  
   - Allows users to confirm or override AI-detected variables and logic  
   - Ideal for ambiguous or inconsistently labeled datasets

3. **Column Typing & Usefulness Classification**  
   - **Useful Columns**: Contain high-value, interpretable info (e.g., name, email, birthdate, hashes)  
   - **Non-Useful Columns**: All others

---

### üßæ Expected Output

A Python script with:
- Pre-assigned variable roles
- Core logic to clean the dataset
- Clear, modifiable sections for user review

---

### üìÅ Status

All projects are actively under development. Contributions and feedback are welcome.

______________________________________________________________________________________________________________________________________________________________________________________________________________________________
# üìä Metrics for Post-Clean Analytics Dashboard (PureView)

**PureView** is a lightweight dashboard designed to visualize key data cleaning metrics, giving teams immediate insight into dataset quality‚Äîwithout reviewing code or logs manually.

The system pulls from post-cleaning script outputs (e.g., duplicates removed, garbage rows, validation success), stores results in a centralized **SQLite** database, and renders interactive dashboards with **Python**.

This approach helps:
- Save time
- Ensure consistency
- Support data quality reporting across teams

---

## üìê Core Metrics

| **Metric**                     | **Raw Dataset** | **Clean Dataset** | **% of Raw** | **Why It Matters**                                                                 |
|--------------------------------|------------------|--------------------|--------------|--------------------------------------------------------------------------------------|
| **Duplicate Count**            | ‚Ä¢                | ‚Ä¢                  | ‚Ä¢            | Verifies deduplication logic (should drop to 0).                                     |
| **Validation Pass**            | ‚Ä¢                | ‚Ä¢                  | ‚Ä¢            | Confirms how much data survives quality checks.                                      |
| **Garbage Records (Rejected)** | ‚Ä¢                | ‚Äì                  | ‚Ä¢            | Quick view of unusable data volume.                                                  |
| **Processing Runtime**         | ‚Äì                | `hh:mm:ss`         | ‚Äì            | Gauges script efficiency for each dataset.                                           |
| **Distinct IDs**               | ‚Ä¢                | ‚Ä¢                  | ‚Äì            | Ensures no duplicate primary keys remain post-clean.                                 |

> ‚ÑπÔ∏è Fill in the ‚Äú‚Ä¢‚Äù programmatically from summary CSVs. Dashes ‚Äú‚Äì‚Äù indicate metrics that don‚Äôt require before/after splits.

---

## üìà Recommended Chart Visuals

| **Metric**            | **Chart Type**                              | **Why This Works**                                                                 |
|-----------------------|---------------------------------------------|-------------------------------------------------------------------------------------|
| Duplicate Count       | Side-by-side bar chart (Raw vs Clean)       | Clearly shows reduction in duplicates across datasets.                             |
| Validation Pass       | Stacked bar (Valid vs Invalid) OR Donut     | Visualizes what portion of data passed validation.                                 |
| Garbage Records       | Donut chart OR Bar chart by dataset         | Emphasizes unusable data and allows for dataset comparison.                        |
| Processing Runtime    | Horizontal bar OR Line chart over time      | Compares runtime per dataset and trends over multiple cleaning runs.               |
| Distinct IDs          | Side-by-side bar chart                      | Shows pre/post-cleaning uniqueness to ensure deduplication worked correctly.        |

---

## ‚ú® Optional Extras (Advanced)

- Tooltip overlays with raw counts & calculated percentages
- Trend lines to show quality improvement across time
- Data Quality Score heatmap for benchmarking across datasets

These extras make the dashboard more insightful for both technical and non-technical users.

---

## üß≠ Recommended Dashboard Layout

### 1Ô∏è‚É£ Summary Table View (Top Section)
A concise overview table displaying:
- Raw vs Clean counts
- Percentages
- Key indicators (runtime, unique IDs)

**Purpose**:  
Provides a high-level snapshot of cleaning performance.

**Bonus**:  
Enable CSV or PDF export for audit trails or business reports.

---

### 2Ô∏è‚É£ Visual Insights (Bottom or Side Section)
Interactive charts tied to each key metric:
- Garbage % ‚Üí Donut chart  
- Runtime ‚Üí Horizontal bar  
- Distinct IDs ‚Üí Side-by-side bar  

**Purpose**:  
Makes trends, outliers, and improvements immediately visible. Useful in stakeholder presentations or data health dashboards.

---

PureView bridges the gap between raw cleaning logs and actionable insights‚Äîtransforming technical outputs into a visual story everyone can understand.

